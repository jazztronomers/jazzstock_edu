{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'numpy.ndarray'>\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n",
      "\n",
      "\n",
      "TARGET\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'numpy.ndarray'>\n",
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n",
      "\n",
      "\n",
      "FEATURE_NAMES\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'numpy.ndarray'>\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "\n",
      "\n",
      "DESCR\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'str'>\n",
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n",
      "\n",
      "\n",
      "FILENAME\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'str'>\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/datasets/data/boston_house_prices.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boston_dataset = load_boston()\n",
    "for k,v in boston_dataset.items():\n",
    "    print(k.upper())\n",
    "    print('-'*80)\n",
    "    print(type(v))\n",
    "    print(v)\n",
    "    print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD DADASET\n",
    "boston_data = boston_dataset['data']                  # NDARRAY\n",
    "boston_feature_names = boston_dataset['feature_names']\n",
    "\n",
    "\n",
    "boston_df = pd.DataFrame(data=boston_data, columns=boston_feature_names)\n",
    "boston_df['price']= boston_dataset['target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
      "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
      "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
      "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
      "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
      "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
      "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
      "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
      "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
      "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
      "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
      "\n",
      "     PTRATIO       B  LSTAT  price  \n",
      "0       15.3  396.90   4.98   24.0  \n",
      "1       17.8  396.90   9.14   21.6  \n",
      "2       17.8  392.83   4.03   34.7  \n",
      "3       18.7  394.63   2.94   33.4  \n",
      "4       18.7  396.90   5.33   36.2  \n",
      "..       ...     ...    ...    ...  \n",
      "501     21.0  391.99   9.67   22.4  \n",
      "502     21.0  396.90   9.08   20.6  \n",
      "503     21.0  396.90   5.64   23.9  \n",
      "504     21.0  393.45   6.48   22.0  \n",
      "505     21.0  396.90   7.88   11.9  \n",
      "\n",
      "[506 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "print(boston_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(boston_df[boston_dataset['feature_names']], boston_df['price'], random_state=0, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7677157140026538\n",
      "0.6705795412578777\n"
     ]
    }
   ],
   "source": [
    "## LINEAR REGRESSION\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "lr = LinearRegression().fit(train_x, train_y)\n",
    "print(lr.score(train_x, train_y))\n",
    "print(lr.score(test_x, test_y))   # LR SCORE : R2 SCORE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 SCORE 에 대한 해석 KR : https://blog.naver.com/lingua/221525365004\n",
    "\n",
    "??lr.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression()\n",
      "TRAIN DATASET SCORE: 0.7677157140026538\n",
      "TEST  DATASET SCORE: 0.6705795412578777\n",
      "TEST  DATASET PREDICTION [25.05724449 24.05407999 29.25938099 12.05750131 21.18581162]\n",
      "TEST  DATASET PREDICTION [25.06, 24.05, 29.26, 12.06, 21.19]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "DecisionTreeRegressor()\n",
      "TRAIN DATASET SCORE: 1.0\n",
      "TEST  DATASET SCORE: 0.6797264596033317\n",
      "TEST  DATASET PREDICTION [23.1 24.7 20.6 19.1 20.8]\n",
      "TEST  DATASET PREDICTION [23.1, 24.7, 20.6, 19.1, 20.8]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "RandomForestRegressor()\n",
      "TRAIN DATASET SCORE: 0.9824684372347832\n",
      "TEST  DATASET SCORE: 0.8184264063244546\n",
      "TEST  DATASET PREDICTION [23.875 27.278 22.167 10.569 20.333]\n",
      "TEST  DATASET PREDICTION [23.87, 27.28, 22.17, 10.57, 20.33]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "GradientBoostingRegressor()\n",
      "TRAIN DATASET SCORE: 0.9851462857317149\n",
      "TEST  DATASET SCORE: 0.8321360510638748\n",
      "TEST  DATASET PREDICTION [24.47906626 31.2547039  23.06190288 10.71701645 21.54919836]\n",
      "TEST  DATASET PREDICTION [24.48, 31.25, 23.06, 10.72, 21.55]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import inspect\n",
    "\n",
    "\n",
    "for eachRegressor in [LinearRegression,\n",
    "                      DecisionTreeRegressor, \n",
    "                      RandomForestRegressor,\n",
    "                      GradientBoostingRegressor]:\n",
    "    \n",
    "    # TRAINING (FIT)\n",
    "    er = eachRegressor().fit(train_x, train_y)\n",
    "    # print(inspect.getsource(eachRegressor().fit))\n",
    "    # VALIDATION (SCORE)\n",
    "    print(er)\n",
    "    print(\"TRAIN DATASET SCORE:\", er.score(train_x, train_y))\n",
    "    print(\"TEST  DATASET SCORE:\", er.score(test_x, test_y))\n",
    "    print(\"TEST  DATASET PREDICTION\", er.predict(test_x[:5]))\n",
    "    \n",
    "    \n",
    "    # ONE LINE PARSING LIST\n",
    "    print(\"TEST  DATASET PREDICTION\", [round(float(prd),2) for prd in  er.predict(test_x[:5])])\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "## https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATASET SCORE: 0.9876008022081595\n",
      "TEST  DATASET SCORE: 0.8386180004096432\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 4,\n",
    "          'learning_rate': 0.01}\n",
    "\n",
    "gbr = GradientBoostingRegressor(**params)\n",
    "gbr = GradientBoostingRegressor(n_estimators=500, max_depth=4, learning_rate=0.01)\n",
    "\n",
    "gbr.fit(train_x, train_y)\n",
    "print(\"TRAIN DATASET SCORE:\", gbr.score(train_x, train_y))\n",
    "print(\"TEST  DATASET SCORE:\", gbr.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN / TEST / 3 / 100 / 0.04 / 0.964 / 0.819\n",
      "TRAIN / TEST / 3 / 100 / 0.01 / 0.772 / 0.677\n",
      "TRAIN / TEST / 3 / 100 / 0.02 / 0.915 / 0.792\n",
      "TRAIN / TEST / 5 / 100 / 0.04 / 0.993 / 0.814\n",
      "TRAIN / TEST / 5 / 100 / 0.01 / 0.830 / 0.672\n",
      "TRAIN / TEST / 5 / 100 / 0.02 / 0.960 / 0.793\n",
      "TRAIN / TEST / 7 / 100 / 0.04 / 0.999 / 0.829\n",
      "TRAIN / TEST / 7 / 100 / 0.01 / 0.853 / 0.664\n",
      "TRAIN / TEST / 7 / 100 / 0.02 / 0.976 / 0.798\n",
      "TRAIN / TEST / 9 / 100 / 0.04 / 1.000 / 0.812\n",
      "TRAIN / TEST / 9 / 100 / 0.01 / 0.863 / 0.658\n",
      "TRAIN / TEST / 9 / 100 / 0.02 / 0.981 / 0.775\n",
      "TRAIN / TEST / 3 / 200 / 0.04 / 0.981 / 0.834\n",
      "TRAIN / TEST / 3 / 200 / 0.01 / 0.914 / 0.788\n",
      "TRAIN / TEST / 3 / 200 / 0.02 / 0.964 / 0.818\n",
      "TRAIN / TEST / 5 / 200 / 0.04 / 0.998 / 0.814\n",
      "TRAIN / TEST / 5 / 200 / 0.01 / 0.960 / 0.793\n",
      "TRAIN / TEST / 5 / 200 / 0.02 / 0.993 / 0.813\n",
      "TRAIN / TEST / 7 / 200 / 0.04 / 1.000 / 0.832\n",
      "TRAIN / TEST / 7 / 200 / 0.01 / 0.976 / 0.798\n",
      "TRAIN / TEST / 7 / 200 / 0.02 / 0.999 / 0.828\n",
      "TRAIN / TEST / 9 / 200 / 0.04 / 1.000 / 0.815\n",
      "TRAIN / TEST / 9 / 200 / 0.01 / 0.981 / 0.787\n",
      "TRAIN / TEST / 9 / 200 / 0.02 / 1.000 / 0.804\n",
      "TRAIN / TEST / 3 / 400 / 0.04 / 0.992 / 0.840\n",
      "TRAIN / TEST / 3 / 400 / 0.01 / 0.964 / 0.817\n",
      "TRAIN / TEST / 3 / 400 / 0.02 / 0.981 / 0.832\n",
      "TRAIN / TEST / 5 / 400 / 0.04 / 1.000 / 0.816\n",
      "TRAIN / TEST / 5 / 400 / 0.01 / 0.993 / 0.817\n",
      "TRAIN / TEST / 5 / 400 / 0.02 / 0.998 / 0.817\n",
      "TRAIN / TEST / 7 / 400 / 0.04 / 1.000 / 0.833\n",
      "TRAIN / TEST / 7 / 400 / 0.01 / 0.998 / 0.825\n",
      "TRAIN / TEST / 7 / 400 / 0.02 / 1.000 / 0.831\n",
      "TRAIN / TEST / 9 / 400 / 0.04 / 1.000 / 0.819\n",
      "TRAIN / TEST / 9 / 400 / 0.01 / 1.000 / 0.813\n",
      "TRAIN / TEST / 9 / 400 / 0.02 / 1.000 / 0.804\n",
      "TRAIN / TEST / 3 / 800 / 0.04 / 0.998 / 0.837\n",
      "TRAIN / TEST / 3 / 800 / 0.01 / 0.981 / 0.834\n",
      "TRAIN / TEST / 3 / 800 / 0.02 / 0.992 / 0.836\n",
      "TRAIN / TEST / 5 / 800 / 0.04 / 1.000 / 0.815\n",
      "TRAIN / TEST / 5 / 800 / 0.01 / 0.998 / 0.823\n",
      "TRAIN / TEST / 5 / 800 / 0.02 / 1.000 / 0.819\n",
      "TRAIN / TEST / 7 / 800 / 0.04 / 1.000 / 0.833\n",
      "TRAIN / TEST / 7 / 800 / 0.01 / 1.000 / 0.830\n",
      "TRAIN / TEST / 7 / 800 / 0.02 / 1.000 / 0.829\n",
      "TRAIN / TEST / 9 / 800 / 0.04 / 1.000 / 0.815\n",
      "TRAIN / TEST / 9 / 800 / 0.01 / 1.000 / 0.815\n",
      "TRAIN / TEST / 9 / 800 / 0.02 / 1.000 / 0.802\n"
     ]
    }
   ],
   "source": [
    "## HYPER PARAMETER TUNING FROM SCRATCH\n",
    "\n",
    "params = {'n_estimators': 400,\n",
    "          'max_depth': 3,\n",
    "          'learning_rate': 0.04,}\n",
    "\n",
    "for ne in [100,200,400,800]:\n",
    "    for md in [3,5,7,9]:\n",
    "        for lr in [0.04, 0.01, 0.02]:\n",
    "            gbr = GradientBoostingRegressor(n_estimators=ne, max_depth=md, learning_rate=lr)\n",
    "            gbr.fit(train_x, train_y)\n",
    "            print(f\"TRAIN / TEST / {md} / {ne} / {lr} / %.3f / %.3f\" %(gbr.score(train_x, train_y), gbr.score(test_x, test_y)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGBoost VS sklearn Comparison\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "params = {'n_estimators': 400,\n",
    "          'max_depth': 3,\n",
    "          'learning_rate': 0.04}\n",
    "\n",
    "\n",
    "gbr = GradientBoostingRegressor(**params)\n",
    "\n",
    "gbr.fit(train_x, train_y)\n",
    "gbr_pred_y= gbr.predict(test_x)\n",
    "\n",
    "\n",
    "xgbr = xgb.XGBRegressor(**params)\n",
    "xgbr.fit(train_x, train_y,\n",
    "         eval_set=[(test_x, test_y)],\n",
    "         eval_metric = 'rmse',\n",
    "         verbose = False)\n",
    "\n",
    "xgb_pred_y = xgbr.predict(test_x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.71619237 31.83116123 22.47646892 10.5871928  21.56501985 20.30638435\n",
      " 21.45057867 20.74162747 22.1219301  18.11060913]\n",
      "[24.282211 27.157354 22.799631 10.612749 21.373272 20.26328  21.288889\n",
      " 20.698767 24.749306 17.425259]\n"
     ]
    }
   ],
   "source": [
    "print(gbr_pred_y[:10])\n",
    "print(xgb_pred_y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.537334366130767 3.9273594461693535\n"
     ]
    }
   ],
   "source": [
    "gbr_rmse = mean_squared_error(test_y, gbr_pred_y, squared=False)\n",
    "xgb_rmse = mean_squared_error(test_y, xgb_pred_y, squared=False)\n",
    "\n",
    "print(gbr_rmse, xgb_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이론 5분\n",
    "## https://www.slideshare.net/freepsw/boosting-bagging-vs-boosting\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
