{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/zhanghang1989/ResNeSt/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
      "Using cache found in /root/.cache/torch/hub/zhanghang1989_ResNeSt_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "        (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (avd_layer): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): SplAtConv2d(\n",
       "        (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2, bias=False)\n",
       "        (bn0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (fc1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (rsoftmax): rSoftMax()\n",
       "      )\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): GlobalAvgPool2d()\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# get list of models\n",
    "torch.hub.list('zhanghang1989/ResNeSt', force_reload=True)\n",
    "# load pretrained models, using ResNeSt-50 as an example\n",
    "model = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an example image from the pytorch website\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.4271e-01, -1.7915e-01, -1.6784e-01,  1.4744e-01, -3.2213e-01,\n",
      "        -4.1240e-02, -1.9824e-02,  2.8830e-01,  7.2640e-01, -1.4942e-01,\n",
      "        -2.8938e-01, -5.2042e-02,  2.7963e-01, -1.4311e-01,  2.8453e-01,\n",
      "        -5.7207e-02, -2.9288e-01, -1.1528e-01, -4.2813e-01, -4.1283e-01,\n",
      "        -1.2124e-01,  2.0144e-02, -7.0579e-02, -7.0398e-01, -5.8025e-01,\n",
      "        -4.2704e-02,  1.3230e-01,  3.9979e-03, -2.0859e-01, -4.9548e-01,\n",
      "        -5.5671e-03,  6.8378e-02,  3.0727e-02, -2.3538e-01, -2.3623e-01,\n",
      "        -2.1838e-01, -1.4642e-02, -3.6024e-02,  1.0105e-02,  3.2280e-02,\n",
      "        -8.9886e-03, -1.3900e-01, -1.7709e-01, -1.5366e-01, -4.2541e-01,\n",
      "        -6.7351e-03,  1.1944e-01, -7.9377e-03,  1.4479e-01, -3.9386e-02,\n",
      "        -7.9602e-02, -8.1164e-02,  6.4051e-04, -2.0356e-01, -1.7461e-01,\n",
      "         2.7610e-03, -3.0141e-01, -4.2637e-02, -3.5680e-01, -8.1847e-02,\n",
      "         7.0873e-02, -7.9611e-02, -8.2599e-03, -3.2718e-01,  2.7305e-01,\n",
      "        -1.9174e-01,  4.9259e-01, -6.5162e-02,  1.8190e-01, -1.8014e-01,\n",
      "         5.8646e-02, -5.6933e-02,  1.0984e-01, -6.2240e-03,  2.0809e-01,\n",
      "        -6.7532e-02, -3.8507e-01,  3.0463e-02,  3.6624e-01,  1.5987e-01,\n",
      "        -3.7049e-01, -2.2234e-01,  3.3140e-01, -2.4840e-01,  6.0906e-01,\n",
      "         3.8393e-01, -1.0711e-01, -5.5135e-01, -5.4772e-01,  5.7631e-01,\n",
      "         8.3360e-01,  2.3592e-01, -2.8959e-01,  3.5509e-01,  1.8240e-01,\n",
      "         2.2503e-01,  4.7945e-03, -1.4470e-01, -3.2587e-01, -2.6730e-02,\n",
      "         3.4812e-01, -1.1999e-01, -1.9340e-01, -3.6343e-01, -3.1408e-02,\n",
      "        -5.2744e-01,  1.1783e-01, -4.6251e-01, -2.4859e-02, -1.8769e-01,\n",
      "        -4.8644e-02,  2.0681e-01,  6.1341e-02,  1.4000e-01, -9.7784e-02,\n",
      "        -3.7118e-01, -1.0538e-01, -2.0574e-01,  1.1513e-01, -1.5224e-01,\n",
      "        -2.3440e-01,  2.3467e-02,  7.3745e-02, -3.9723e-02,  1.2942e-01,\n",
      "        -2.4933e-01, -8.1320e-02,  1.0413e+00, -3.9525e-01, -5.7098e-02,\n",
      "        -3.3139e-01,  3.9006e-01, -6.4352e-01, -2.5039e-01, -5.4615e-01,\n",
      "         1.2951e+00, -2.9912e-01, -8.1084e-01, -3.1639e-01,  3.4516e-01,\n",
      "         1.1549e+00,  8.9650e-01,  5.0988e-01, -1.1132e-01, -2.7577e-01,\n",
      "        -2.1423e-01,  5.2469e-01, -4.4845e-02, -3.5198e-02,  2.1024e-01,\n",
      "        -4.6177e-01,  1.1967e+00,  1.8516e+00,  8.6462e-01,  1.5190e+00,\n",
      "        -9.9379e-01, -6.3231e-01,  7.1477e-01, -7.9252e-01, -1.9842e-01,\n",
      "        -8.5585e-01, -5.5388e-01, -5.3420e-01,  7.3318e-02,  6.3825e-01,\n",
      "         8.8223e-02,  1.8030e-01, -6.7738e-01, -2.8339e-01,  5.8990e-02,\n",
      "        -1.2147e+00, -9.9023e-01, -6.3075e-01, -2.4679e-01, -5.8681e-02,\n",
      "        -2.5873e-01, -2.1788e-01, -4.7913e-01, -2.6952e-02,  6.7431e-02,\n",
      "        -7.5013e-02, -1.0900e+00, -1.7501e-02, -6.0957e-02, -7.0052e-01,\n",
      "        -5.8901e-01,  3.7825e-02, -9.0232e-01, -8.6570e-01, -1.1621e+00,\n",
      "        -5.7158e-01, -2.3446e-01, -1.5325e-01, -9.3573e-01,  5.6842e-01,\n",
      "        -3.5405e-02, -6.1017e-01,  3.2646e-01, -8.4486e-01, -4.8777e-01,\n",
      "         7.5391e-01, -1.9748e-01, -5.5127e-01,  4.6718e-01,  4.5802e-01,\n",
      "        -1.3044e+00, -8.9367e-01, -7.9233e-02, -3.1095e-01, -5.8812e-01,\n",
      "         2.2118e-01,  3.4788e-01,  2.4356e-01, -1.9308e-01, -5.0232e-02,\n",
      "        -1.9447e-01, -2.9632e-02, -1.0699e-01, -1.9839e-01, -7.0885e-01,\n",
      "         2.0352e-01, -3.3610e-01,  1.5997e+00,  1.6858e+00,  8.1593e-01,\n",
      "        -6.1086e-01,  4.8778e-01, -1.1363e+00,  6.5050e-01, -5.8311e-01,\n",
      "        -4.3294e-02,  1.0342e+00,  3.8386e-02, -3.4534e-01, -8.4786e-01,\n",
      "         2.5527e-01,  7.7133e-01,  3.4421e-01, -2.6776e-01, -1.6304e+00,\n",
      "        -2.6076e-01, -3.7941e-01,  7.1610e-01, -2.6722e-01, -1.0036e+00,\n",
      "        -1.1899e+00,  4.5252e-01, -3.8266e-02,  2.3119e+00,  6.5289e-01,\n",
      "         2.8507e-01,  8.2958e-02,  3.7441e-01, -4.9504e-01, -1.6612e+00,\n",
      "        -5.3477e-01, -7.1152e-01,  1.9453e+00,  7.9376e+00,  6.3491e+00,\n",
      "         2.1030e+00,  4.7039e+00,  4.6429e-02, -8.8141e-01, -1.2616e+00,\n",
      "         4.4719e-01, -1.4904e-01,  4.0099e-01, -3.6243e-01,  3.0076e-01,\n",
      "         2.8244e+00, -1.0305e-01, -1.0409e-02, -3.3216e-01, -2.9506e-01,\n",
      "        -1.0584e+00, -3.8534e-02, -3.3703e-01, -3.2710e-01,  2.6035e+00,\n",
      "         2.5486e-01, -6.2440e-01, -5.1018e-01,  1.4705e+00, -1.0497e+00,\n",
      "         7.3363e-03, -9.3162e-01,  3.4060e-01,  6.7289e-02,  1.2018e-01,\n",
      "        -2.3189e-01, -3.3324e-01, -2.0539e-01, -4.2445e-01, -5.6827e-01,\n",
      "        -5.8576e-01, -2.8154e-01, -5.3213e-02, -6.6685e-02, -5.0978e-01,\n",
      "         2.4854e-02,  6.8477e-02,  9.1042e-02, -6.5659e-02,  5.5267e-02,\n",
      "        -1.2496e-03,  7.1365e-02,  1.2043e-01,  1.7414e-01, -5.9342e-01,\n",
      "         3.5861e-02,  3.1071e-02, -1.0712e-03,  3.1113e-02,  2.2782e-01,\n",
      "        -2.6207e-01,  1.7281e-01,  1.9507e-01, -1.0624e-01, -1.3197e-01,\n",
      "         6.7054e-02,  5.9574e-02, -2.3466e-01,  1.1767e-01, -7.9100e-02,\n",
      "        -1.4975e-01,  5.1499e-04, -5.7486e-02, -2.6778e-01, -2.5741e-01,\n",
      "        -3.6459e-01, -3.8354e-02,  3.8647e-01,  6.9618e-02,  5.0663e-01,\n",
      "         1.1666e+00, -5.3628e-01, -3.5809e-01, -6.4797e-01,  2.0325e-01,\n",
      "        -2.8020e-02, -7.8275e-01, -2.2244e-01, -1.1471e-01, -1.1060e-01,\n",
      "        -1.7176e-01, -1.7659e-01,  7.2721e-02, -2.6498e-01, -3.1301e-01,\n",
      "        -1.8289e-01,  5.5434e-02,  1.0692e-01, -2.2596e-02, -2.4566e-01,\n",
      "         4.4450e-01, -1.6561e-01,  1.3004e-01,  7.4052e-02, -6.4360e-01,\n",
      "        -2.0854e-01,  4.8271e-01, -7.6828e-01, -1.3427e-01, -2.8799e-01,\n",
      "         3.8770e-01, -6.5636e-02, -1.9060e-01, -2.8372e-01, -1.9944e-01,\n",
      "         5.4196e-02, -5.4028e-01,  1.4574e-01, -4.0221e-01,  6.2468e-02,\n",
      "        -2.2806e-01,  3.8733e-02,  5.8117e-01,  3.8462e-01,  1.2164e-01,\n",
      "        -6.9429e-01, -3.7994e-01, -9.0085e-01, -4.6728e-01, -1.0499e+00,\n",
      "        -1.3254e-01,  2.7128e-02,  4.3504e-01, -4.8677e-01, -1.4903e-01,\n",
      "        -4.7304e-02,  1.4805e-01, -1.1847e-01, -3.0269e-02, -1.5091e-01,\n",
      "         1.5834e-01, -5.9049e-02, -1.8942e-01, -6.2748e-03,  3.0686e-02,\n",
      "         1.0178e-02, -4.7662e-02,  1.7825e-01, -2.7030e-01, -1.2361e-02,\n",
      "         3.2137e-01,  5.2129e-02, -2.1592e-01, -1.4007e-01,  2.1342e-01,\n",
      "         2.5312e-01, -1.6315e-02,  4.5785e-01,  3.0194e-01, -2.1902e-01,\n",
      "        -1.7643e-01,  2.9634e-02,  1.6577e-01,  7.0006e-02, -1.6017e-01,\n",
      "         3.0569e-02, -3.0651e-01,  1.1344e-02,  2.0099e-01,  9.6228e-02,\n",
      "         6.2263e-02,  1.8803e-01,  4.0758e-02,  1.3453e-01, -2.1549e-02,\n",
      "        -2.6862e-02,  2.5618e-01,  1.7891e-01,  6.7726e-02, -2.3064e-01,\n",
      "         2.0222e-03, -7.0989e-02,  1.5487e-01,  1.1049e-01,  6.5831e-02,\n",
      "        -9.4287e-03,  4.9457e-02, -2.1658e-01, -1.9048e-01,  2.0663e-01,\n",
      "         8.6365e-02,  4.4646e-01, -7.3960e-02,  7.2690e-02,  1.9854e-01,\n",
      "         1.4153e-01, -1.2815e-01, -2.5454e-02,  6.3533e-02, -1.4806e-01,\n",
      "         3.3910e-02,  3.2248e-01, -8.1183e-02, -1.9397e-01,  3.8682e-03,\n",
      "        -1.2524e-01,  3.7183e-01,  8.0037e-02,  1.0814e-01, -2.6656e-02,\n",
      "         1.6424e-01,  4.0732e-02, -9.4921e-02, -3.1760e-02, -2.2976e-02,\n",
      "        -4.4156e-01,  6.9677e-02, -1.9302e-01,  2.1952e-01, -2.2105e-01,\n",
      "         2.4363e-01, -1.6273e-01, -1.5904e-01,  4.8185e-01, -1.4264e-01,\n",
      "        -1.8009e-01,  1.6387e-02, -2.5766e-01, -4.1854e-02, -7.6370e-02,\n",
      "        -3.0522e-02,  1.3818e-01,  2.7563e-02, -3.9140e-02, -8.2755e-03,\n",
      "         2.9628e-01,  2.3325e-01, -1.0155e-01,  2.2917e-01,  2.7112e-01,\n",
      "         1.2032e-01, -6.2547e-01, -4.4958e-01, -1.7118e-01, -8.6809e-02,\n",
      "        -3.8769e-01,  3.0752e-01,  8.0110e-03, -1.9118e-02, -3.4071e-01,\n",
      "         1.0874e-01,  9.6501e-03,  2.1517e-01,  1.5496e-01, -1.5669e-02,\n",
      "         1.8608e-01,  7.6599e-02,  3.0117e-01,  6.1462e-02, -2.3092e-01,\n",
      "         1.4169e-01, -4.2271e-02, -1.4510e-01, -2.6316e-01, -1.8726e-01,\n",
      "        -3.3452e-01,  1.5404e-01,  2.6692e-01, -1.9247e-01,  3.9080e-01,\n",
      "        -2.9354e-01,  2.0081e-01, -2.0244e-01,  1.5559e-01, -1.3993e-01,\n",
      "         1.4058e-01, -9.9647e-02,  3.8620e-02, -4.7592e-02,  2.7759e-02,\n",
      "        -2.1103e-02, -6.1241e-01,  8.2729e-02, -2.7727e-01,  4.3239e-01,\n",
      "        -3.3265e-04,  1.1250e-01, -2.9163e-01, -5.7844e-02, -1.4615e-01,\n",
      "        -2.1146e-02,  2.2542e-02, -1.1924e-01,  3.6339e-01,  7.1635e-02,\n",
      "         7.8105e-02, -1.1463e-01, -5.3494e-01,  5.8421e-02, -1.6263e-02,\n",
      "        -2.4483e-01, -1.6030e-02,  2.7687e-01,  6.0308e-02,  3.4979e-01,\n",
      "        -1.2286e-01,  1.2742e-01, -3.6728e-01,  6.7883e-02, -2.6517e-01,\n",
      "        -1.5699e-01,  6.6501e-02, -8.8279e-02, -1.0896e-02, -3.2450e-02,\n",
      "         2.4654e-02,  9.6636e-02, -1.3903e-01,  1.1742e-01, -8.2733e-02,\n",
      "         2.4924e-01, -2.1768e-01,  9.7103e-02,  1.0327e-01, -8.9038e-02,\n",
      "        -6.2809e-02,  1.0327e-01, -1.0331e-01, -1.6828e-01,  2.8857e-01,\n",
      "        -1.3510e-01, -6.0380e-02, -1.6430e-01, -4.5073e-03, -8.9191e-02,\n",
      "         1.3100e-01,  8.2867e-02, -1.1639e-01,  2.4101e-01,  1.2297e-01,\n",
      "         2.2847e-01,  4.4617e-01, -2.3553e-02,  6.5529e-02,  2.2099e-01,\n",
      "         3.7391e-02,  2.9425e-01, -3.0062e-02,  1.2827e-01,  1.0253e-01,\n",
      "         1.3136e-01,  1.5407e-01,  1.8435e-01, -2.3149e-01,  6.8868e-02,\n",
      "        -2.2529e-01, -9.1044e-02, -4.3482e-01, -6.3500e-02,  1.3552e-01,\n",
      "         1.6411e-01,  9.5940e-02, -2.5640e-01,  1.0398e-02, -2.1374e-01,\n",
      "        -2.1046e-01,  4.5473e-01,  1.5041e-01,  2.6913e-02, -1.9275e-01,\n",
      "        -4.3422e-02, -5.9265e-02,  1.6528e-01, -2.0992e-02,  1.8504e-01,\n",
      "        -4.7271e-02, -8.2914e-02, -5.5142e-02, -1.0233e-01, -1.7172e-01,\n",
      "        -2.7112e-01, -1.4196e-01,  1.5597e-02,  9.6243e-03, -5.5851e-02,\n",
      "        -2.7486e-01, -3.0299e-01,  1.3456e-01,  1.4431e-01,  4.2943e-01,\n",
      "         8.3520e-02,  2.1143e-01,  7.3330e-02,  6.9657e-02,  1.1425e-01,\n",
      "        -2.7059e-01,  1.1994e-01, -8.3064e-02,  4.0287e-02,  2.5460e-01,\n",
      "        -1.8892e-01, -1.9919e-01, -1.4968e-02, -1.3369e-01,  1.3796e-01,\n",
      "         2.5213e-01, -9.2593e-02,  1.7528e-01,  1.5532e-01, -1.3329e-01,\n",
      "         1.5131e-01, -4.1033e-02,  1.5673e-01,  8.0678e-02,  9.1551e-02,\n",
      "         5.2025e-02,  6.0319e-02,  1.9340e-01,  4.9472e-02,  1.2179e-02,\n",
      "        -3.5302e-02,  9.7632e-02,  3.3512e-01, -2.5525e-01, -3.6732e-01,\n",
      "        -2.4554e-01,  3.9474e-01,  2.7462e-02,  1.4132e-01, -4.3298e-01,\n",
      "         1.1678e-01, -2.7145e-02, -2.3207e-01,  5.7302e-02,  5.4014e-01,\n",
      "        -1.2098e-01, -2.7882e-01, -2.4101e-01, -3.4394e-01, -9.4187e-02,\n",
      "        -1.7144e-02, -3.2492e-01,  4.3948e-01,  1.9051e-02,  5.1376e-02,\n",
      "        -1.7762e-01, -1.1770e-01, -2.7791e-01,  1.9935e-01, -1.6635e-02,\n",
      "        -6.2452e-02,  3.3143e-01, -1.1076e-01,  5.5144e-01, -1.9754e-01,\n",
      "        -2.3480e-01,  7.7065e-02, -7.7751e-03,  5.7751e-01, -1.7842e-01,\n",
      "         3.0782e-03,  4.1625e-01,  1.4851e-01,  1.4161e-01, -3.7624e-02,\n",
      "        -2.9651e-02, -6.7513e-02, -1.3171e-01,  1.2151e-01,  1.0108e-01,\n",
      "        -1.6789e-01,  2.8728e-01, -1.4467e-01, -1.2156e-01,  9.7542e-02,\n",
      "        -1.7715e-01, -4.2355e-03,  2.1698e-01, -1.9600e-01,  1.0160e-01,\n",
      "        -1.5631e-01, -2.7122e-01,  7.7950e-02, -7.2179e-02, -5.6012e-02,\n",
      "        -1.0176e-01,  1.4370e-01,  1.6243e-01, -4.6145e-01, -2.2724e-02,\n",
      "         1.7182e-01, -8.4609e-02,  1.7749e-01,  6.3535e-02, -9.2867e-02,\n",
      "        -2.6129e-02, -2.5054e-02,  8.5654e-03,  2.3894e-01, -2.7543e-02,\n",
      "        -4.8437e-02,  9.3244e-02,  1.0395e-01, -1.2269e-01,  3.0449e-01,\n",
      "        -1.7882e-01, -1.7629e-01, -2.3961e-02,  5.4110e-01, -6.6962e-02,\n",
      "         1.3310e-01,  1.9578e-01,  9.9283e-02,  1.6889e-01,  2.3023e-01,\n",
      "         4.3313e-02, -1.1182e-01, -3.9064e-02, -2.4121e-01,  2.8473e-01,\n",
      "         6.3255e-02,  8.6550e-02, -1.6442e-03,  1.0961e-01, -1.0451e-01,\n",
      "         2.6198e-01,  1.1434e-01,  7.2003e-02,  5.7721e-02, -1.7752e-01,\n",
      "         7.8299e-03,  5.6899e-02, -1.5485e-01, -1.0018e-01,  3.7101e-02,\n",
      "        -3.0102e-01, -2.8470e-01, -2.9882e-01, -3.0190e-01, -3.6287e-01,\n",
      "        -3.2961e-01, -2.6363e-01, -1.9096e-01,  3.0582e-01,  4.6001e-02,\n",
      "         4.6766e-02,  1.5346e-01, -1.5103e-01,  5.8739e-02, -1.6082e-01,\n",
      "         9.6110e-02, -3.2805e-02,  8.0931e-02,  1.3056e-01, -4.7537e-01,\n",
      "         1.9608e-01,  2.4796e-01,  1.7951e-01, -1.6809e-01,  4.1816e-02,\n",
      "        -6.2450e-02,  5.2800e-02,  4.1423e-02, -4.4438e-02,  4.8699e-03,\n",
      "        -1.6629e-01, -8.9332e-02,  1.1487e-01,  5.3424e-01, -4.6554e-01,\n",
      "         1.4245e-01, -5.9074e-03,  9.2366e-02,  1.5879e-01,  3.7952e-02,\n",
      "         1.3379e-01,  2.4225e-01,  1.6209e-01, -1.3291e-01,  2.6994e-01,\n",
      "         2.6464e-01,  7.0062e-02,  3.1665e-01, -3.8730e-01, -9.6404e-02,\n",
      "        -1.4085e-01, -2.5682e-01, -2.9548e-01,  2.1605e-01, -2.0435e-01,\n",
      "        -6.6132e-02,  4.1427e-02,  1.2256e-01,  1.6076e-01, -6.1450e-02,\n",
      "        -5.9331e-01,  2.2451e-01,  1.2070e+00, -9.0993e-02,  1.9080e-01,\n",
      "        -2.0278e-01,  8.4581e-02, -2.3841e-02,  6.5817e-01,  1.0122e-01,\n",
      "         1.5172e-01,  1.7374e-01, -9.7310e-02,  3.2225e-01,  2.1384e-02,\n",
      "        -4.0594e-01,  6.9235e-02, -8.8476e-02, -1.2777e-01,  1.4689e-02,\n",
      "         1.9733e-01,  4.6132e-01,  6.7110e-02, -5.3446e-02, -2.1864e-01,\n",
      "         7.9347e-02,  1.0699e-02, -2.1576e-01,  1.2808e-01,  2.8027e-02,\n",
      "        -5.6354e-02,  8.4665e-02, -3.4957e-02, -5.5637e-01, -3.0883e-01,\n",
      "        -3.9371e-02, -1.2161e-01,  1.2951e-01, -9.2655e-02, -1.5913e-01,\n",
      "        -1.9961e-01,  2.2616e-01, -1.2123e-01, -9.1259e-02, -2.4920e-02,\n",
      "        -1.1505e-01, -1.2334e-01,  1.5275e-01, -9.8333e-02, -2.6742e-01,\n",
      "        -2.0069e-01,  1.9329e-02,  1.1908e-01, -1.5126e-01,  9.5259e-02,\n",
      "         1.9648e-01,  1.2123e-01, -1.4574e-01, -8.5934e-02, -1.1576e-01,\n",
      "         3.1432e-01,  2.1407e-01,  3.6075e-01, -1.2688e-01,  1.9978e-01,\n",
      "         1.7717e-01,  6.7634e-02,  6.9211e-02, -8.5169e-02,  1.8617e-02,\n",
      "         6.6712e-03,  3.6089e-03,  1.1587e-01,  3.7525e-01,  3.1722e-01,\n",
      "        -1.5499e-01, -1.4513e-01, -9.8502e-02, -3.7660e-01,  3.8094e-01,\n",
      "        -9.2009e-02, -5.8472e-02,  7.8874e-02,  1.6275e-01, -2.9687e-02,\n",
      "         5.7353e-02,  4.4625e-03,  5.0569e-02, -7.3474e-02, -5.8217e-02,\n",
      "         1.3524e-01, -9.4012e-02, -2.5801e-01, -1.1859e-01,  5.4457e-02,\n",
      "         2.9023e-01, -1.9886e-02, -6.1410e-01,  8.1990e-02,  2.9054e-01,\n",
      "        -9.7340e-02,  1.2133e-01,  1.6018e-01,  7.5142e-02, -1.2933e-01,\n",
      "         3.9454e-02,  8.8778e-02,  2.1188e-01,  3.4032e-01, -1.3381e-01,\n",
      "         2.1928e-01, -1.4491e-01,  2.3781e-01,  1.5854e-01, -1.7371e-01,\n",
      "        -3.2832e-01, -6.8967e-02,  5.4635e-02,  4.3148e-01, -8.6628e-02,\n",
      "        -2.5266e-01,  6.1719e-02,  2.6041e-02, -2.0398e-01,  9.9481e-02,\n",
      "        -3.6716e-01,  3.0718e-02,  4.9446e-01,  9.0300e-02, -1.8689e-02,\n",
      "        -1.3520e-01, -3.7332e-02, -6.9171e-03,  1.7636e-01, -1.4655e-01,\n",
      "        -1.8628e-01,  2.0143e-01,  4.6049e-02,  2.8595e-01,  8.1923e-02,\n",
      "         6.6005e-01, -1.0071e-02, -1.1529e-01, -8.0845e-02, -5.4643e-03,\n",
      "        -2.2317e-01,  4.7521e-02, -2.6112e-01,  2.5278e-01, -3.8042e-01])\n",
      "tensor([2.7943e-04, 1.8326e-04, 1.8534e-04, 2.5404e-04, 1.5884e-04, 2.1035e-04,\n",
      "        2.1491e-04, 2.9246e-04, 4.5324e-04, 1.8879e-04, 1.6413e-04, 2.0809e-04,\n",
      "        2.8994e-04, 1.8998e-04, 2.9136e-04, 2.0702e-04, 1.6356e-04, 1.9534e-04,\n",
      "        1.4287e-04, 1.4507e-04, 1.9418e-04, 2.2367e-04, 2.0427e-04, 1.0842e-04,\n",
      "        1.2270e-04, 2.1005e-04, 2.5022e-04, 2.2009e-04, 1.7794e-04, 1.3356e-04,\n",
      "        2.1799e-04, 2.3472e-04, 2.2605e-04, 1.7324e-04, 1.7309e-04, 1.7621e-04,\n",
      "        2.1602e-04, 2.1145e-04, 2.2144e-04, 2.2640e-04, 2.1725e-04, 1.9076e-04,\n",
      "        1.8363e-04, 1.8799e-04, 1.4326e-04, 2.1774e-04, 2.4702e-04, 2.1748e-04,\n",
      "        2.5336e-04, 2.1074e-04, 2.0244e-04, 2.0212e-04, 2.1935e-04, 1.7884e-04,\n",
      "        1.8409e-04, 2.1982e-04, 1.6217e-04, 2.1006e-04, 1.5343e-04, 2.0198e-04,\n",
      "        2.3531e-04, 2.0244e-04, 2.1741e-04, 1.5804e-04, 2.8804e-04, 1.8096e-04,\n",
      "        3.5875e-04, 2.0538e-04, 2.6294e-04, 1.8307e-04, 2.3245e-04, 2.0708e-04,\n",
      "        2.4466e-04, 2.1785e-04, 2.6992e-04, 2.0490e-04, 1.4915e-04, 2.2599e-04,\n",
      "        3.1617e-04, 2.5721e-04, 1.5134e-04, 1.7551e-04, 3.0534e-04, 1.7100e-04,\n",
      "        4.0306e-04, 3.2181e-04, 1.9694e-04, 1.2630e-04, 1.2676e-04, 3.9008e-04,\n",
      "        5.0453e-04, 2.7754e-04, 1.6409e-04, 3.1266e-04, 2.6307e-04, 2.7453e-04,\n",
      "        2.2026e-04, 1.8968e-04, 1.5825e-04, 2.1343e-04, 3.1049e-04, 1.9442e-04,\n",
      "        1.8066e-04, 1.5241e-04, 2.1243e-04, 1.2936e-04, 2.4662e-04, 1.3804e-04,\n",
      "        2.1383e-04, 1.8170e-04, 2.0880e-04, 2.6957e-04, 2.3308e-04, 2.5215e-04,\n",
      "        1.9879e-04, 1.5124e-04, 1.9729e-04, 1.7845e-04, 2.4596e-04, 1.8825e-04,\n",
      "        1.7340e-04, 2.2442e-04, 2.3599e-04, 2.1067e-04, 2.4950e-04, 1.7084e-04,\n",
      "        2.0209e-04, 6.2100e-04, 1.4764e-04, 2.0704e-04, 1.5738e-04, 3.2379e-04,\n",
      "        1.1518e-04, 1.7065e-04, 1.2696e-04, 8.0043e-04, 1.6254e-04, 9.7435e-05,\n",
      "        1.5975e-04, 3.0957e-04, 6.9568e-04, 5.3729e-04, 3.6500e-04, 1.9612e-04,\n",
      "        1.6638e-04, 1.7694e-04, 3.7045e-04, 2.0960e-04, 2.1163e-04, 2.7050e-04,\n",
      "        1.3814e-04, 7.2540e-04, 1.3964e-03, 5.2043e-04, 1.0013e-03, 8.1146e-05,\n",
      "        1.1648e-04, 4.4801e-04, 9.9238e-05, 1.7976e-04, 9.3147e-05, 1.2598e-04,\n",
      "        1.2849e-04, 2.3589e-04, 4.1500e-04, 2.3943e-04, 2.6252e-04, 1.1135e-04,\n",
      "        1.6512e-04, 2.3253e-04, 6.5064e-05, 8.1435e-05, 1.1666e-04, 1.7127e-04,\n",
      "        2.0672e-04, 1.6924e-04, 1.7629e-04, 1.3576e-04, 2.1338e-04, 2.3450e-04,\n",
      "        2.0337e-04, 7.3704e-05, 2.1541e-04, 2.0625e-04, 1.0880e-04, 1.2163e-04,\n",
      "        2.2766e-04, 8.8918e-05, 9.2234e-05, 6.8574e-05, 1.2377e-04, 1.7340e-04,\n",
      "        1.8806e-04, 8.5996e-05, 3.8701e-04, 2.1159e-04, 1.1909e-04, 3.0384e-04,\n",
      "        9.4177e-05, 1.3459e-04, 4.6589e-04, 1.7993e-04, 1.2631e-04, 3.4975e-04,\n",
      "        3.4656e-04, 5.9479e-05, 8.9690e-05, 2.0251e-04, 1.6063e-04, 1.2174e-04,\n",
      "        2.7348e-04, 3.1042e-04, 2.7967e-04, 1.8072e-04, 2.0847e-04, 1.8047e-04,\n",
      "        2.1281e-04, 1.9697e-04, 1.7976e-04, 1.0790e-04, 2.6869e-04, 1.5664e-04,\n",
      "        1.0854e-03, 1.1830e-03, 4.9570e-04, 1.1901e-04, 3.5703e-04, 7.0366e-05,\n",
      "        4.2011e-04, 1.2235e-04, 2.0992e-04, 6.1663e-04, 2.2779e-04, 1.5520e-04,\n",
      "        9.3895e-05, 2.8296e-04, 4.7407e-04, 3.0928e-04, 1.6772e-04, 4.2934e-05,\n",
      "        1.6889e-04, 1.5000e-04, 4.4860e-04, 1.6781e-04, 8.0354e-05, 6.6697e-05,\n",
      "        3.4466e-04, 2.1098e-04, 2.2127e-03, 4.2112e-04, 2.9152e-04, 2.3817e-04,\n",
      "        3.1876e-04, 1.3362e-04, 4.1629e-05, 1.2841e-04, 1.0761e-04, 1.5335e-03,\n",
      "        6.1393e-01, 1.2538e-01, 1.7956e-03, 2.4196e-02, 2.2963e-04, 9.0797e-05,\n",
      "        6.2081e-05, 3.4283e-04, 1.8886e-04, 3.2735e-04, 1.5257e-04, 2.9613e-04,\n",
      "        3.6939e-03, 1.9775e-04, 2.1694e-04, 1.5725e-04, 1.6320e-04, 7.6067e-05,\n",
      "        2.1092e-04, 1.5649e-04, 1.5805e-04, 2.9616e-03, 2.8284e-04, 1.1741e-04,\n",
      "        1.3161e-04, 9.5391e-04, 7.6733e-05, 2.2082e-04, 8.6350e-05, 3.0816e-04,\n",
      "        2.3447e-04, 2.4720e-04, 1.7384e-04, 1.5709e-04, 1.7851e-04, 1.4339e-04,\n",
      "        1.2418e-04, 1.2203e-04, 1.6542e-04, 2.0785e-04, 2.0507e-04, 1.3166e-04,\n",
      "        2.2473e-04, 2.3475e-04, 2.4010e-04, 2.0528e-04, 2.3167e-04, 2.1894e-04,\n",
      "        2.3543e-04, 2.4727e-04, 2.6091e-04, 1.2110e-04, 2.2721e-04, 2.2613e-04,\n",
      "        2.1898e-04, 2.2614e-04, 2.7530e-04, 1.6867e-04, 2.6056e-04, 2.6643e-04,\n",
      "        1.9712e-04, 1.9211e-04, 2.3441e-04, 2.3267e-04, 1.7336e-04, 2.4658e-04,\n",
      "        2.0254e-04, 1.8872e-04, 2.1932e-04, 2.0696e-04, 1.6771e-04, 1.6946e-04,\n",
      "        1.5224e-04, 2.1096e-04, 3.2263e-04, 2.3502e-04, 3.6382e-04, 7.0388e-04,\n",
      "        1.2822e-04, 1.5323e-04, 1.1467e-04, 2.6862e-04, 2.1315e-04, 1.0021e-04,\n",
      "        1.7549e-04, 1.9545e-04, 1.9626e-04, 1.8461e-04, 1.8373e-04, 2.3575e-04,\n",
      "        1.6818e-04, 1.6030e-04, 1.8257e-04, 2.3171e-04, 2.4395e-04, 2.1431e-04,\n",
      "        1.7146e-04, 3.4191e-04, 1.8575e-04, 2.4965e-04, 2.3606e-04, 1.1517e-04,\n",
      "        1.7795e-04, 3.5522e-04, 1.0167e-04, 1.9167e-04, 1.6436e-04, 3.2302e-04,\n",
      "        2.0528e-04, 1.8117e-04, 1.6506e-04, 1.7957e-04, 2.3142e-04, 1.2771e-04,\n",
      "        2.5360e-04, 1.4662e-04, 2.3334e-04, 1.7451e-04, 2.2787e-04, 3.9198e-04,\n",
      "        3.2203e-04, 2.4756e-04, 1.0948e-04, 1.4992e-04, 8.9049e-05, 1.3738e-04,\n",
      "        7.6721e-05, 1.9200e-04, 2.2524e-04, 3.3868e-04, 1.3473e-04, 1.8886e-04,\n",
      "        2.0908e-04, 2.5419e-04, 1.9472e-04, 2.1267e-04, 1.8850e-04, 2.5682e-04,\n",
      "        2.0664e-04, 1.8138e-04, 2.1784e-04, 2.2604e-04, 2.2145e-04, 2.0901e-04,\n",
      "        2.6198e-04, 1.6729e-04, 2.1652e-04, 3.0229e-04, 2.3094e-04, 1.7664e-04,\n",
      "        1.9056e-04, 2.7136e-04, 2.8235e-04, 2.1566e-04, 3.4650e-04, 2.9648e-04,\n",
      "        1.7609e-04, 1.8375e-04, 2.2580e-04, 2.5874e-04, 2.3511e-04, 1.8677e-04,\n",
      "        2.2602e-04, 1.6134e-04, 2.2171e-04, 2.6801e-04, 2.4135e-04, 2.3329e-04,\n",
      "        2.6456e-04, 2.2833e-04, 2.5078e-04, 2.1454e-04, 2.1340e-04, 2.8322e-04,\n",
      "        2.6216e-04, 2.3457e-04, 1.7406e-04, 2.1965e-04, 2.0419e-04, 2.5593e-04,\n",
      "        2.4482e-04, 2.3413e-04, 2.1715e-04, 2.3032e-04, 1.7652e-04, 1.8119e-04,\n",
      "        2.6953e-04, 2.3898e-04, 3.4257e-04, 2.0358e-04, 2.3574e-04, 2.6735e-04,\n",
      "        2.5254e-04, 1.9284e-04, 2.1370e-04, 2.3359e-04, 1.8904e-04, 2.2677e-04,\n",
      "        3.0263e-04, 2.0212e-04, 1.8056e-04, 2.2006e-04, 1.9341e-04, 3.1794e-04,\n",
      "        2.3748e-04, 2.4425e-04, 2.1344e-04, 2.5834e-04, 2.2832e-04, 1.9936e-04,\n",
      "        2.1236e-04, 2.1423e-04, 1.4096e-04, 2.3503e-04, 1.8073e-04, 2.7302e-04,\n",
      "        1.7574e-04, 2.7968e-04, 1.8629e-04, 1.8698e-04, 3.5492e-04, 1.9007e-04,\n",
      "        1.8308e-04, 2.2283e-04, 1.6942e-04, 2.1023e-04, 2.0309e-04, 2.1262e-04,\n",
      "        2.5169e-04, 2.2534e-04, 2.1080e-04, 2.1740e-04, 2.9480e-04, 2.7680e-04,\n",
      "        1.9804e-04, 2.7567e-04, 2.8748e-04, 2.4724e-04, 1.1728e-04, 1.3983e-04,\n",
      "        1.8472e-04, 2.0098e-04, 1.4876e-04, 2.9814e-04, 2.2097e-04, 2.1506e-04,\n",
      "        1.5592e-04, 2.4439e-04, 2.2134e-04, 2.7184e-04, 2.5595e-04, 2.1580e-04,\n",
      "        2.6404e-04, 2.3666e-04, 2.9625e-04, 2.3311e-04, 1.7401e-04, 2.5258e-04,\n",
      "        2.1014e-04, 1.8960e-04, 1.6849e-04, 1.8178e-04, 1.5689e-04, 2.5572e-04,\n",
      "        2.8627e-04, 1.8083e-04, 3.2403e-04, 1.6345e-04, 2.6796e-04, 1.7904e-04,\n",
      "        2.5611e-04, 1.9059e-04, 2.5230e-04, 1.9842e-04, 2.2784e-04, 2.0902e-04,\n",
      "        2.2538e-04, 2.1463e-04, 1.1882e-04, 2.3812e-04, 1.6613e-04, 3.3779e-04,\n",
      "        2.1914e-04, 2.4531e-04, 1.6376e-04, 2.0689e-04, 1.8940e-04, 2.1462e-04,\n",
      "        2.2421e-04, 1.9457e-04, 3.1527e-04, 2.3549e-04, 2.3702e-04, 1.9547e-04,\n",
      "        1.2839e-04, 2.3240e-04, 2.1567e-04, 1.7161e-04, 2.1572e-04, 2.8914e-04,\n",
      "        2.3284e-04, 3.1101e-04, 1.9387e-04, 2.4900e-04, 1.5183e-04, 2.3461e-04,\n",
      "        1.6815e-04, 1.8736e-04, 2.3428e-04, 2.0069e-04, 2.1683e-04, 2.1221e-04,\n",
      "        2.2468e-04, 2.4145e-04, 1.9076e-04, 2.4652e-04, 2.0180e-04, 2.8126e-04,\n",
      "        1.7633e-04, 2.4156e-04, 2.4306e-04, 2.0054e-04, 2.0587e-04, 2.4306e-04,\n",
      "        1.9769e-04, 1.8526e-04, 2.9254e-04, 1.9151e-04, 2.0637e-04, 1.8600e-04,\n",
      "        2.1822e-04, 2.0051e-04, 2.4989e-04, 2.3815e-04, 1.9513e-04, 2.7895e-04,\n",
      "        2.4789e-04, 2.7548e-04, 3.4248e-04, 2.1411e-04, 2.3406e-04, 2.7342e-04,\n",
      "        2.2756e-04, 2.9421e-04, 2.1272e-04, 2.4921e-04, 2.4288e-04, 2.4998e-04,\n",
      "        2.5572e-04, 2.6359e-04, 1.7391e-04, 2.3484e-04, 1.7499e-04, 2.0013e-04,\n",
      "        1.4191e-04, 2.0572e-04, 2.5102e-04, 2.5830e-04, 2.4128e-04, 1.6963e-04,\n",
      "        2.2150e-04, 1.7703e-04, 1.7761e-04, 3.4542e-04, 2.5479e-04, 2.2519e-04,\n",
      "        1.8078e-04, 2.0990e-04, 2.0660e-04, 2.5861e-04, 2.1466e-04, 2.6377e-04,\n",
      "        2.0909e-04, 2.0177e-04, 2.0745e-04, 1.9789e-04, 1.8462e-04, 1.6715e-04,\n",
      "        1.9020e-04, 2.2266e-04, 2.2133e-04, 2.0730e-04, 1.6653e-04, 1.6191e-04,\n",
      "        2.5078e-04, 2.5324e-04, 3.3679e-04, 2.3831e-04, 2.7082e-04, 2.3589e-04,\n",
      "        2.3502e-04, 2.4574e-04, 1.6724e-04, 2.4714e-04, 2.0174e-04, 2.2822e-04,\n",
      "        2.8277e-04, 1.8147e-04, 1.7962e-04, 2.1595e-04, 1.9178e-04, 2.5164e-04,\n",
      "        2.8207e-04, 1.9982e-04, 2.6121e-04, 2.5605e-04, 1.9186e-04, 2.5502e-04,\n",
      "        2.1040e-04, 2.5641e-04, 2.3763e-04, 2.4023e-04, 2.3092e-04, 2.3284e-04,\n",
      "        2.6598e-04, 2.3033e-04, 2.2190e-04, 2.1161e-04, 2.4169e-04, 3.0648e-04,\n",
      "        1.6983e-04, 1.5182e-04, 1.7148e-04, 3.2531e-04, 2.2531e-04, 2.5249e-04,\n",
      "        1.4217e-04, 2.4637e-04, 2.1334e-04, 1.7381e-04, 2.3214e-04, 3.7622e-04,\n",
      "        1.9423e-04, 1.6587e-04, 1.7226e-04, 1.5541e-04, 1.9951e-04, 2.1548e-04,\n",
      "        1.5840e-04, 3.4019e-04, 2.2343e-04, 2.3077e-04, 1.8354e-04, 1.9487e-04,\n",
      "        1.6602e-04, 2.6757e-04, 2.1559e-04, 2.0594e-04, 3.0535e-04, 1.9623e-04,\n",
      "        3.8049e-04, 1.7992e-04, 1.7334e-04, 2.3677e-04, 2.1751e-04, 3.9055e-04,\n",
      "        1.8339e-04, 2.1989e-04, 3.3238e-04, 2.5431e-04, 2.5256e-04, 2.1112e-04,\n",
      "        2.1281e-04, 2.0490e-04, 1.9216e-04, 2.4753e-04, 2.4253e-04, 1.8533e-04,\n",
      "        2.9216e-04, 1.8968e-04, 1.9412e-04, 2.4167e-04, 1.8362e-04, 2.1828e-04,\n",
      "        2.7233e-04, 1.8019e-04, 2.4265e-04, 1.8749e-04, 1.6714e-04, 2.3698e-04,\n",
      "        2.0395e-04, 2.0727e-04, 1.9800e-04, 2.5309e-04, 2.5787e-04, 1.3818e-04,\n",
      "        2.1429e-04, 2.6030e-04, 2.0143e-04, 2.6179e-04, 2.3359e-04, 1.9977e-04,\n",
      "        2.1356e-04, 2.1379e-04, 2.2110e-04, 2.7838e-04, 2.1326e-04, 2.0885e-04,\n",
      "        2.4063e-04, 2.4322e-04, 1.9390e-04, 2.9723e-04, 1.8332e-04, 1.8378e-04,\n",
      "        2.1402e-04, 3.7658e-04, 2.0501e-04, 2.5042e-04, 2.6662e-04, 2.4209e-04,\n",
      "        2.5954e-04, 2.7596e-04, 2.2891e-04, 1.9602e-04, 2.1081e-04, 1.7223e-04,\n",
      "        2.9142e-04, 2.3352e-04, 2.3903e-04, 2.1885e-04, 2.4460e-04, 1.9746e-04,\n",
      "        2.8487e-04, 2.4576e-04, 2.3558e-04, 2.3224e-04, 1.8355e-04, 2.2093e-04,\n",
      "        2.3204e-04, 1.8776e-04, 1.9831e-04, 2.2750e-04, 1.6223e-04, 1.6490e-04,\n",
      "        1.6259e-04, 1.6209e-04, 1.5250e-04, 1.5766e-04, 1.6841e-04, 1.8110e-04,\n",
      "        2.9763e-04, 2.2953e-04, 2.2971e-04, 2.5557e-04, 1.8848e-04, 2.3247e-04,\n",
      "        1.8664e-04, 2.4132e-04, 2.1214e-04, 2.3769e-04, 2.4978e-04, 1.3627e-04,\n",
      "        2.6670e-04, 2.8090e-04, 2.6231e-04, 1.8529e-04, 2.2857e-04, 2.0594e-04,\n",
      "        2.3110e-04, 2.2848e-04, 2.0968e-04, 2.2028e-04, 1.8563e-04, 2.0048e-04,\n",
      "        2.4589e-04, 3.7401e-04, 1.3762e-04, 2.5277e-04, 2.1792e-04, 2.4042e-04,\n",
      "        2.5693e-04, 2.2769e-04, 2.5059e-04, 2.7930e-04, 2.5778e-04, 1.9193e-04,\n",
      "        2.8714e-04, 2.8562e-04, 2.3512e-04, 3.0087e-04, 1.4882e-04, 1.9906e-04,\n",
      "        1.9041e-04, 1.6956e-04, 1.6313e-04, 2.7208e-04, 1.7870e-04, 2.0518e-04,\n",
      "        2.2848e-04, 2.4779e-04, 2.5744e-04, 2.0615e-04, 1.2111e-04, 2.7439e-04,\n",
      "        7.3294e-04, 2.0014e-04, 2.6529e-04, 1.7898e-04, 2.3856e-04, 2.1405e-04,\n",
      "        4.2335e-04, 2.4256e-04, 2.5513e-04, 2.6080e-04, 1.9888e-04, 3.0256e-04,\n",
      "        2.2395e-04, 1.4607e-04, 2.3493e-04, 2.0065e-04, 1.9292e-04, 2.2245e-04,\n",
      "        2.6703e-04, 3.4771e-04, 2.3443e-04, 2.0780e-04, 1.7616e-04, 2.3731e-04,\n",
      "        2.2157e-04, 1.7667e-04, 2.4916e-04, 2.2544e-04, 2.0720e-04, 2.3858e-04,\n",
      "        2.1168e-04, 1.2567e-04, 1.6097e-04, 2.1075e-04, 1.9411e-04, 2.4952e-04,\n",
      "        1.9981e-04, 1.8696e-04, 1.7955e-04, 2.7484e-04, 1.9418e-04, 2.0009e-04,\n",
      "        2.1382e-04, 1.9539e-04, 1.9377e-04, 2.5539e-04, 1.9868e-04, 1.6777e-04,\n",
      "        1.7935e-04, 2.2349e-04, 2.4693e-04, 1.8844e-04, 2.4112e-04, 2.6680e-04,\n",
      "        2.4746e-04, 1.8948e-04, 2.0116e-04, 1.9525e-04, 3.0017e-04, 2.7154e-04,\n",
      "        3.1444e-04, 1.9309e-04, 2.6769e-04, 2.6170e-04, 2.3455e-04, 2.3492e-04,\n",
      "        2.0131e-04, 2.2333e-04, 2.2068e-04, 2.2000e-04, 2.4614e-04, 3.1903e-04,\n",
      "        3.0104e-04, 1.8774e-04, 1.8960e-04, 1.9865e-04, 1.5042e-04, 3.2085e-04,\n",
      "        1.9994e-04, 2.0676e-04, 2.3720e-04, 2.5795e-04, 2.1280e-04, 2.3215e-04,\n",
      "        2.2019e-04, 2.3058e-04, 2.0368e-04, 2.0681e-04, 2.5096e-04, 1.9954e-04,\n",
      "        1.6936e-04, 1.9470e-04, 2.3148e-04, 2.9303e-04, 2.1489e-04, 1.1862e-04,\n",
      "        2.3794e-04, 2.9312e-04, 1.9888e-04, 2.4749e-04, 2.5729e-04, 2.3632e-04,\n",
      "        1.9262e-04, 2.2803e-04, 2.3956e-04, 2.7094e-04, 3.0808e-04, 1.9175e-04,\n",
      "        2.7296e-04, 1.8964e-04, 2.7806e-04, 2.5687e-04, 1.8426e-04, 1.5786e-04,\n",
      "        2.0460e-04, 2.3152e-04, 3.3748e-04, 2.0102e-04, 1.7027e-04, 2.3317e-04,\n",
      "        2.2499e-04, 1.7876e-04, 2.4214e-04, 1.5185e-04, 2.2605e-04, 3.5942e-04,\n",
      "        2.3993e-04, 2.1515e-04, 1.9149e-04, 2.1118e-04, 2.1770e-04, 2.6149e-04,\n",
      "        1.8933e-04, 1.8195e-04, 2.6813e-04, 2.2954e-04, 2.9177e-04, 2.3792e-04,\n",
      "        4.2415e-04, 2.1701e-04, 1.9534e-04, 2.0219e-04, 2.1802e-04, 1.7536e-04,\n",
      "        2.2988e-04, 1.6883e-04, 2.8226e-04, 1.4985e-04])\n"
     ]
    }
   ],
   "source": [
    "# sample execution (requires torchvision)\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "input_image = Image.open(filename)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "print(torch.nn.functional.softmax(output[0], dim=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "def save_image(numpy_array, name):\n",
    "    '''\n",
    "    numpy array 형태의 이미지를 받아서 png파일로 저장하는 함수\n",
    "    \n",
    "    '''\n",
    "    matplotlib.image.imsave(name, numpy_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "im = Image.fromarray(output[0].numpy())\n",
    "if im.mode != 'RGB':\n",
    "    im = im.convert('RGB')\n",
    "im.save(\"your_file.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
