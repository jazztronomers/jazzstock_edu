{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns= 500\n",
    "pd.options.display.expand_frame_repr=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습데이터 csv파일 경로 정의\n",
    "datatag = 'bikedemand'\n",
    "train_data_path = 'data/%s_train.csv'%(datatag)\n",
    "test_data_path  = 'data/%s_test.csv'%(datatag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV파일을 pandas dataframe으로 불러오기\n",
    "\n",
    "df_train = pd.read_csv(train_data_path)\n",
    "df_test = pd.read_csv(test_data_path) # BLIND DATA, FOR SUBMISSION\n",
    "\n",
    "# PRINT WHOLE COLUMNS\n",
    "print(df_train.columns)\n",
    "print(df_train)\n",
    "\n",
    "\n",
    "# DEFINE FEATURE, TARGET COLUMNS\n",
    "feature = ['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed']\n",
    "target = ['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT PREPROCESSING\n",
    "train_x, test_x, train_y, test_y = train_test_split(df_train[feature], \n",
    "                                                    df_train[target], \n",
    "                                                    random_state=0, \n",
    "                                                    test_size=0.25)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# GRID SEARCH MANUALY\n",
    "# 3 * 3 * 2\n",
    "for md in [5,7,9]:\n",
    "    for ne in [200,400,600]:\n",
    "        for lr in [0.02, 0.03]:\n",
    "            gbm = xgb.XGBRegressor(# tree_method='gpu_hist', \n",
    "                                                    objective='reg:squarederror', \n",
    "                                                    booster='gbtree', \n",
    "                                                    max_depth=md, \n",
    "                                                    n_estimators=ne, \n",
    "                                                    learning_rate=lr, \n",
    "                                                    random_state=1)\n",
    "            gbm = gbm.fit(train_x[feature], train_y[target], \n",
    "                  eval_set=[(test_x[feature], test_y[target])],\n",
    "                  eval_metric = ['gamma-deviance', 'rmse', 'mae'],\n",
    "                  verbose=False)\n",
    "\n",
    "\n",
    "            evals_result = gbm.evals_result()\n",
    "            df_eval = pd.DataFrame(evals_result['validation_0'])\n",
    "            print(ne, md, lr, df_eval.rmse.min(), df_eval.mae.min())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "\n",
    "# 전처리는 재활용목적으로 미리미리 함수로 작성해둔다.\n",
    "def preprocessing(df_input): \n",
    "    df = df_input.copy() \n",
    "    \n",
    "    df[\"hour\"] = [t.hour for t in pd.DatetimeIndex(df.datetime)]\n",
    "    df[\"day\"] = [t.dayofweek for t in pd.DatetimeIndex(df.datetime)]\n",
    "    df[\"month\"] = [t.month for t in pd.DatetimeIndex(df.datetime)]\n",
    "    df['year'] = [t.year for t in pd.DatetimeIndex(df.datetime)]\n",
    "    df['year'] = df['year'].map({2011:0, 2012:1})\n",
    "\n",
    "    \n",
    "    season=pd.get_dummies(df['season'],prefix='season')\n",
    "    df=pd.concat([df,season],axis=1)\n",
    "\n",
    "    weather=pd.get_dummies(df['weather'],prefix='weather')\n",
    "    df=pd.concat([df,weather],axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df_train_preprocessed = preprocessing(df_train)\n",
    "df_test_preprocessed = preprocessing(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK NEW GENERATED FEATURE COLUMNS\n",
    "print(df_train.describe())\n",
    "print('-'*100)\n",
    "print(df_train_preprocessed.describe())\n",
    "\n",
    "\n",
    "print(df_train.head(5))\n",
    "print(df_train_preprocessed.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = ['season', 'holiday', 'workingday', 'weather', 'temp','atemp', 'humidity', 'windspeed', 'hour', 'day', 'month', 'year', 'season_1', 'season_2', 'season_3','season_4', 'weather_1', 'weather_2', 'weather_3', 'weather_4']\n",
    "target = ['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(df_train_preprocessed[feature], \n",
    "                                                    df_train_preprocessed[target], \n",
    "                                                    random_state=0, \n",
    "                                                    test_size=0.25)\n",
    "\n",
    "\n",
    "\n",
    "# GRID SEARCH MANUALLLY\n",
    "for md in [5,7,9]:\n",
    "    for ne in [200,400,600]:\n",
    "        for lr in [0.02, 0.03]:\n",
    "            gbm = xgb.XGBRegressor(# tree_method='gpu_hist', \n",
    "                                                    objective='reg:squarederror', \n",
    "                                                    booster='gbtree', \n",
    "                                                    max_depth=md, \n",
    "                                                    n_estimators=ne, \n",
    "                                                    learning_rate=lr, \n",
    "                                                    random_state=1)\n",
    "            gbm = gbm.fit(train_x[feature], train_y[target], \n",
    "                  eval_set=[(test_x[feature], test_y[target])],\n",
    "                  eval_metric = ['rmsle', 'gamma-deviance', 'rmse', 'mae'],\n",
    "                  verbose=False)\n",
    "\n",
    "            evals_result = gbm.evals_result()\n",
    "            df_eval = pd.DataFrame(evals_result['validation_0'])\n",
    "            print(ne, md, lr, df_eval.rmsle.min(), df_eval.rmse.min(), df_eval.mae.min(), float(df_eval.rmse.min())**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID SEARCH BY GRIDSEARCHCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': [5,7,9],\n",
    "              'n_estimators':[200,400,600],\n",
    "              'learning_rate':[0.02,0.03]}\n",
    "             # 'max_features':['auto', 'sqrt', 'log2']\n",
    "             \n",
    "\n",
    "\n",
    "# GRID SEARCH WITH CROSS VALIDATION\n",
    "# cross validation 은 트레이닝 데이터셋을 균일하게 n 개로 나눠서 서로 교차검증 하므로, train test split 을 할 필요 없다\n",
    "\n",
    "# CHECK AVALABLE SCORING FUNCTION\n",
    "# for e in sorted(sklearn.metrics.SCORERS.keys()):\n",
    "#     print(e)\n",
    "    \n",
    "    \n",
    "    \n",
    "search = GridSearchCV(xgb.XGBRegressor(), param_grid, cv=2, scoring='neg_mean_squared_error')\n",
    "search.fit(train_x[feature], train_y[target])\n",
    "\n",
    "# PRINT THE BEST\n",
    "\n",
    "print('%-40s: %s'%('BEST ESTIMATOR:', search.best_estimator_))\n",
    "print('%-40s: %s'%('BEST SCORE:', search.best_score_))\n",
    "print('%-40s: %s'%('BEST INDEX:', search.best_index_))\n",
    "print('%-40s: %s'%('BEST PARAMS:', search.best_params_))\n",
    "\n",
    "# print('='*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수동 결과와 CV 결과 비교하기\n",
    "# 사실은 CV 할꺼면 BLIND DATASET 을 만들어서 따로 하는게 맞음... 시간 있으면 나누자\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "pred_y = search.predict(test_x[feature])\n",
    "print(mean_squared_error(pred_y, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAYESIAN OPTIMIZATION !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID SEARCH 를 자동으로 진행하는 방법.\n",
    "# 이론상세는 다음 링크를 참조\n",
    "# https://www.kdnuggets.com/2019/07/xgboost-random-forest-bayesian-optimisation.html\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_optimization(dataset, function, parameters):\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = dataset\n",
    "    n_iterations = 5\n",
    "    gp_params = {\"alpha\": 1e-4}\n",
    "    BO = BayesianOptimization(function, parameters)\n",
    "    BO.maximize(n_iter=n_iterations, **gp_params)\n",
    "\n",
    "    return BO.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_optimization(cv_splits, eval_set):\n",
    "    def function(eta, max_depth, n_estimators):\n",
    "            return cross_val_score(\n",
    "                   xgb.XGBRegressor(\n",
    "                        objective='reg:squarederror',\n",
    "                       n_estimators=int(max(n_estimators,0)),\n",
    "                       learning_rate=max(learning_rate, 0),\n",
    "                       max_depth=int(max_depth),                                               \n",
    "                       seed=42,\n",
    "                       nthread=-1,),  \n",
    "                   X=train_x, \n",
    "                   y=train_y, \n",
    "                   cv=cv_splits,\n",
    "                   scoring=\"neg_mean_squared_error\",\n",
    "                   fit_params={\n",
    "                        \"early_stopping_rounds\": 10, \n",
    "                        \"eval_metric\": \"rmse\", \n",
    "                        \"eval_set\": eval_set},\n",
    "                   n_jobs=-1).mean()\n",
    "\n",
    "    # ========================================================\n",
    "    # 아래에 TUNING을 진행할 PARAMS의 범위를 설정\n",
    "    # ========================================================\n",
    "    parameters = {\"learning_rate\": (0.001, 0.04),\n",
    "                  \"max_depth\": (2, 12),\n",
    "                  \"n_estimators\":(100,2000)}\n",
    "    \n",
    "    return function, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_x, test_x, train_y, test_y, function, parameters):\n",
    "    dataset = (train_x, test_x, train_y, test_y)\n",
    "    cv_splits = 5\n",
    "    \n",
    "    best_solution = bayesian_optimization(dataset, function, parameters)      \n",
    "    params = best_solution[\"params\"]\n",
    "    \n",
    "    # ======================================================================================\n",
    "    # BAYESIAN OPTIMIZATION으로 얻은 최적의 HYPER PARAMETER 출력 !\n",
    "    # ======================================================================================\n",
    "    for k,v in params.items():\n",
    "        print(k,v)\n",
    "        \n",
    "    md = int(params['max_depth'])\n",
    "    ne = int(params['n_estimators'])\n",
    "    lr = params['learning_rate']\n",
    "\n",
    "    \n",
    "    # ======================================================================================\n",
    "    # BAYESIAN OPTIMIZATION으로 얻은 최적의 HYPER PARAMETER 로 학습 진행, SCORE 출력\n",
    "    # ======================================================================================\n",
    "    gbm = xgb.XGBRegressor(# tree_method='gpu_hist', \n",
    "                                        objective='reg:squarederror', \n",
    "                                        max_depth=md, \n",
    "                                        n_estimators=ne, \n",
    "                                        learning_rate=lr, \n",
    "                                        random_state=1)\n",
    "    gbm = gbm.fit(train_x[feature], train_y[target], \n",
    "      eval_set=[(test_x[feature], test_y[target])],\n",
    "      eval_metric = ['rmsle', 'gamma-deviance', 'rmse', 'mae'],\n",
    "      verbose=False)\n",
    "\n",
    "    evals_result = gbm.evals_result()\n",
    "    df_eval = pd.DataFrame(evals_result['validation_0'])\n",
    "    print(ne, md, lr, df_eval.rmsle.min(), df_eval.rmse.min(), df_eval.mae.min(), float(df_eval.rmse.min())**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수정의\n",
    "\n",
    "train(train_x, test_x, train_y, test_y, *xgb_optimization(5,[(test_x[feature], test_y[target])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "## MACHINE LEARNING SUMMARY!\n",
    "# ======================================================================================\n",
    "\n",
    "\n",
    "1) 무엇보다 \"모델튜닝\"보다 전처리가 \"훨씬\" 중요하다.\n",
    "2) 알고리즘 별 적절한 SCORING METHOD를 골라줘야 한다. (구글링 키워드 : scoring metrics)\n",
    "3) xgboost는 max_depth, n_estimators, learning_rate 외에도 많은 params를 튜닝할 수 있다, \n",
    "   공식 document를 참조하여 각 params의 의미를 파악하고 적절한 parameter를 선택한다\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
